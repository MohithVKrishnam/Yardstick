{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eXUfzVtlHDpX"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet openai\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "MODE = \"mock\"\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "if MODE == \"live\" and not OPENAI_API_KEY:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in env/Colab before switching to live mode\")\n",
        "\n",
        "import openai\n",
        "if OPENAI_API_KEY:\n",
        "    openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self):\n",
        "        self.history: List[Message] = []\n",
        "        self.run_counter = 0\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.history.append(Message(role=role, content=content))\n",
        "\n",
        "    def get_last_n_turns(self, n: int) -> List[Message]:\n",
        "        return self.history[-n*2:] if n > 0 else []\n",
        "\n",
        "    def truncate_by_chars(self, max_chars: int) -> List[Message]:\n",
        "        kept = []\n",
        "        chars = 0\n",
        "        for msg in reversed(self.history):\n",
        "            if chars + len(msg.content) > max_chars:\n",
        "                break\n",
        "            kept.append(msg)\n",
        "            chars += len(msg.content)\n",
        "        return list(reversed(kept))\n",
        "\n",
        "    def summarize_history_with_llm(self, prompt_extra: str = \"\") -> str:\n",
        "        combined = \"\\n\\n\".join([f\"{m.role}: {m.content}\" for m in self.history])\n",
        "        prompt = (\n",
        "            \"Summarize the following user-assistant conversation in 2-4 concise sentences, \"\n",
        "            \"preserving key facts. \" + prompt_extra + \"\\n\\nConversation:\\n\" + combined\n",
        "        )\n",
        "        if MODE == \"mock\":\n",
        "            snippet = \" | \".join([m.content[:60].replace(\"\\n\", \" \") for m in self.history[-6:]])\n",
        "            summary = f\"(MOCK SUMMARY) Recent points: {snippet}\"\n",
        "            return summary\n",
        "        else:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                max_tokens=200,\n",
        "                temperature=0.2\n",
        "            )\n",
        "            text = resp.choices[0].message.content.strip()\n",
        "            return text\n",
        "\n",
        "    def periodic_summarize(self, k: int):\n",
        "        self.run_counter += 1\n",
        "        if self.run_counter % k == 0:\n",
        "            summary = self.summarize_history_with_llm(f\"Perform periodic summary at run {self.run_counter}.\")\n",
        "            self.history = [Message(role=\"assistant\", content=f\"[SUMMARY after run {self.run_counter}]\\n{summary}\")]\n",
        "            return summary\n",
        "        return None\n",
        "\n",
        "    def get_history_text(self):\n",
        "        return \"\\n\".join([f\"{m.role}: {m.content}\" for m in self.history])\n"
      ],
      "metadata": {
        "id": "wCu9I5AaHdHt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConversationManager()\n",
        "\n",
        "sample_pairs = [\n",
        "    (\"user\",\"Hi, I'm looking for help with my ML model evaluation.\"),\n",
        "    (\"assistant\",\"Sure — what's the model and data like?\"),\n",
        "    (\"user\",\"It's a classification model trained on images; performance drops on new lighting.\"),\n",
        "    (\"assistant\",\"Consider data augmentation, normalization; also check distribution shift.\"),\n",
        "    (\"user\",\"How to implement augmentation in PyTorch?\"),\n",
        "    (\"assistant\",\"You can use torchvision transforms; here's a sample snippet...\"),\n",
        "]\n",
        "\n",
        "for role, text in sample_pairs:\n",
        "    cm.add_message(role, text)\n",
        "\n",
        "print(\"=== Full history ===\")\n",
        "print(cm.get_history_text())\n",
        "\n",
        "last_2_turns = cm.get_last_n_turns(2)\n",
        "print(\"\\n=== Last 2 turns (approx) ===\")\n",
        "for m in last_2_turns:\n",
        "    print(f\"{m.role}: {m.content}\")\n",
        "\n",
        "trunc = cm.truncate_by_chars(200)\n",
        "print(\"\\n=== Truncated by 200 chars ===\")\n",
        "for m in trunc:\n",
        "    print(f\"{m.role}: {m.content}\")\n",
        "\n",
        "print(\"\\n=== Periodic summarization demo ===\")\n",
        "for i in range(1,6):\n",
        "    cm.add_message(\"user\", f\"New question #{i}: example followup info.\")\n",
        "    cm.add_message(\"assistant\", f\"Assistant reply #{i}.\")\n",
        "    summary = cm.periodic_summarize(k=3)\n",
        "    if summary:\n",
        "        print(f\"\\n--- Summarized at run {cm.run_counter} ---\")\n",
        "        print(summary)\n",
        "        print(\"\\nCurrent history after summarization:\")\n",
        "        print(cm.get_history_text())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2CsB5CINhd",
        "outputId": "9e4a50b2-ded0-47e4-e15f-26e1d987a711"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Full history ===\n",
            "user: Hi, I'm looking for help with my ML model evaluation.\n",
            "assistant: Sure — what's the model and data like?\n",
            "user: It's a classification model trained on images; performance drops on new lighting.\n",
            "assistant: Consider data augmentation, normalization; also check distribution shift.\n",
            "user: How to implement augmentation in PyTorch?\n",
            "assistant: You can use torchvision transforms; here's a sample snippet...\n",
            "\n",
            "=== Last 2 turns (approx) ===\n",
            "user: It's a classification model trained on images; performance drops on new lighting.\n",
            "assistant: Consider data augmentation, normalization; also check distribution shift.\n",
            "user: How to implement augmentation in PyTorch?\n",
            "assistant: You can use torchvision transforms; here's a sample snippet...\n",
            "\n",
            "=== Truncated by 200 chars ===\n",
            "assistant: Consider data augmentation, normalization; also check distribution shift.\n",
            "user: How to implement augmentation in PyTorch?\n",
            "assistant: You can use torchvision transforms; here's a sample snippet...\n",
            "\n",
            "=== Periodic summarization demo ===\n",
            "\n",
            "--- Summarized at run 3 ---\n",
            "(MOCK SUMMARY) Recent points: New question #1: example followup info. | Assistant reply #1. | New question #2: example followup info. | Assistant reply #2. | New question #3: example followup info. | Assistant reply #3.\n",
            "\n",
            "Current history after summarization:\n",
            "assistant: [SUMMARY after run 3]\n",
            "(MOCK SUMMARY) Recent points: New question #1: example followup info. | Assistant reply #1. | New question #2: example followup info. | Assistant reply #2. | New question #3: example followup info. | Assistant reply #3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "25FmRUeMIlof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCHEMA = {\n",
        "    \"name\": {\"type\": \"string\", \"required\": True},\n",
        "    \"email\": {\"type\": \"string\", \"required\": True},\n",
        "    \"phone\": {\"type\": \"string\", \"required\": False},\n",
        "    \"location\": {\"type\": \"string\", \"required\": False},\n",
        "    \"age\": {\"type\": \"integer\", \"required\": False}\n",
        "}\n",
        "\n",
        "def simple_validate_schema(result: dict, schema: dict) -> (bool, list):\n",
        "    errors = []\n",
        "    for key, meta in schema.items():\n",
        "        if meta.get(\"required\") and key not in result:\n",
        "            errors.append(f\"Missing required field: {key}\")\n",
        "        if key in result:\n",
        "            t = type(result[key])\n",
        "            expected = meta[\"type\"]\n",
        "            if expected == \"integer\" and not isinstance(result[key], int):\n",
        "                try:\n",
        "                    result[key] = int(result[key])\n",
        "                except Exception:\n",
        "                    errors.append(f\"Field {key} expected integer but got {t}\")\n",
        "            if expected == \"string\" and not isinstance(result[key], str):\n",
        "                errors.append(f\"Field {key} expected string but got {t}\")\n",
        "    return (len(errors) == 0, errors)\n",
        "\n",
        "sample_chats = [\n",
        "    \"Hi, I'm Alice Johnson. My email is alice.johnson@mail.com and I live in Wonderland. I'm 25 years old. Contact: 12345-irrelevant\",\n",
        "    \"Hello, name: Bob Smith; email:bob.smith@gmail.com. Phone: xyz-0000. Based in Metropolis.\",\n",
        "    \"User: Charlie Brown. Email: charlie.brown@example.com. No phone. Age 28.\"\n",
        "]\n",
        "\n",
        "def extract_with_llm(chat_text: str) -> dict:\n",
        "    if MODE == \"mock\":\n",
        "        out = {\"name\": None, \"email\": None, \"phone\": None, \"location\": None, \"age\": None}\n",
        "        import re\n",
        "        m = re.search(r'[\\w\\.-]+@[\\w\\.-]+', chat_text)\n",
        "        if m: out[\"email\"] = m.group(0)\n",
        "        p = re.search(r'\\+?\\d[\\d\\-\\s]{7,}\\d', chat_text)\n",
        "        if p: out[\"phone\"] = p.group(0).replace(\" \", \"\")\n",
        "        age = re.search(r'(\\bage\\b[: ]*|I\\'m |I am )(\\d{1,2})', chat_text, flags=re.I)\n",
        "        if age:\n",
        "            out[\"age\"] = int(age.group(2))\n",
        "        name = re.search(r\"(I'm|I am|name[: ]|User:)\\s*([A-Z][\\w\\s\\.']{2,40})\", chat_text)\n",
        "        if name:\n",
        "            out[\"name\"] = name.group(2).strip(\" .;\")\n",
        "        loc = re.search(r'in ([A-Za-z ]+)\\.?', chat_text)\n",
        "        if loc:\n",
        "            out[\"location\"] = loc.group(1).strip()\n",
        "        if not out[\"name\"]:\n",
        "            first_tokens = chat_text.split()[:3]\n",
        "            out[\"name\"] = \" \".join(first_tokens)\n",
        "        return out\n",
        "    else:\n",
        "        function_def = {\n",
        "            \"name\": \"extract_info\",\n",
        "            \"description\": \"Extract contact details into JSON per provided schema\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\"type\": \"string\"},\n",
        "                    \"email\": {\"type\": \"string\"},\n",
        "                    \"phone\": {\"type\": \"string\"},\n",
        "                    \"location\": {\"type\": \"string\"},\n",
        "                    \"age\": {\"type\": \"integer\"},\n",
        "                },\n",
        "                \"required\": [\"name\", \"email\"]\n",
        "            }\n",
        "        }\n",
        "        messages = [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful extractor. Return only JSON via function calling.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"Extract fields from this chat: ```{chat_text}```\"}\n",
        "        ]\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            functions=[function_def],\n",
        "            function_call={\"name\":\"extract_info\"},\n",
        "            temperature=0\n",
        "        )\n",
        "        func_call = resp.choices[0].message.get(\"function_call\", {})\n",
        "        args = func_call.get(\"arguments\", \"{}\")\n",
        "        try:\n",
        "            parsed = json.loads(args)\n",
        "        except:\n",
        "            parsed = {}\n",
        "        return parsed\n",
        "\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    extracted = extract_with_llm(chat)\n",
        "    valid, errors = simple_validate_schema(extracted, SCHEMA)\n",
        "    print(f\"\\nSample chat #{i}:\")\n",
        "    print(chat)\n",
        "    print(\"Extracted JSON:\", json.dumps(extracted, indent=2))\n",
        "    print(\"Validation:\", \"OK\" if valid else \"FAIL\", errors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZzdYADMIWD-",
        "outputId": "6e3f5e26-13ae-4fe4-d1f2-0d576dec36d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample chat #1:\n",
            "Hi, I'm Alice Johnson. My email is alice.johnson@mail.com and I live in Wonderland. I'm 25 years old. Contact: 12345-irrelevant\n",
            "Extracted JSON: {\n",
            "  \"name\": \"Alice Johnson. My email is alice.johnson\",\n",
            "  \"email\": \"alice.johnson@mail.com\",\n",
            "  \"phone\": null,\n",
            "  \"location\": \"Wonderland\",\n",
            "  \"age\": 25\n",
            "}\n",
            "Validation: FAIL [\"Field phone expected string but got <class 'NoneType'>\"]\n",
            "\n",
            "Sample chat #2:\n",
            "Hello, name: Bob Smith; email:bob.smith@gmail.com. Phone: xyz-0000. Based in Metropolis.\n",
            "Extracted JSON: {\n",
            "  \"name\": \"Bob Smith\",\n",
            "  \"email\": \"bob.smith@gmail.com.\",\n",
            "  \"phone\": null,\n",
            "  \"location\": \"Metropolis\",\n",
            "  \"age\": null\n",
            "}\n",
            "Validation: FAIL [\"Field phone expected string but got <class 'NoneType'>\", \"Field age expected integer but got <class 'NoneType'>\"]\n",
            "\n",
            "Sample chat #3:\n",
            "User: Charlie Brown. Email: charlie.brown@example.com. No phone. Age 28.\n",
            "Extracted JSON: {\n",
            "  \"name\": \"Charlie Brown. Email\",\n",
            "  \"email\": \"charlie.brown@example.com.\",\n",
            "  \"phone\": null,\n",
            "  \"location\": null,\n",
            "  \"age\": 28\n",
            "}\n",
            "Validation: FAIL [\"Field phone expected string but got <class 'NoneType'>\", \"Field location expected string but got <class 'NoneType'>\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPByuw7sJr0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}